{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e7df47-6fdf-4a3b-9b33-422da180abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0    1     2     3   4     5   6     7    8      9   \\\n",
      "0    2006.01.01 14:05:00  0.7  0.90  1.67  -1 -0.29   3  16.8 -2.1  503.0   \n",
      "1    2006.01.04 20:00:00  0.7  1.10  0.84   7 -0.16   9   7.0 -4.5  359.0   \n",
      "2    2006.01.06 07:00:00  0.6  0.94  0.86  69 -0.20   9   8.5 -5.4  401.0   \n",
      "3    2006.01.10 17:00:00  0.6  0.81  0.70  -2 -0.16   9   5.8 -4.8  336.0   \n",
      "4    2006.01.15 14:00:00  1.3  1.04  1.30  28 -0.24   9  12.3 -6.1  459.0   \n",
      "..                   ...  ...   ...   ...  ..   ...  ..   ...  ...    ...   \n",
      "104  2020.12.19 09:00:00  0.5  1.05  0.87  11 -0.21   9   9.0 -3.3  388.0   \n",
      "105  2020.12.20 23:00:00  0.6  0.86  0.92  19 -0.18   9  11.6 -6.5  407.0   \n",
      "106  2020.12.22 01:00:00  0.6  0.82  0.72   6 -0.20   9  12.9 -2.4  637.0   \n",
      "107  2020.12.27 04:00:00  0.5  0.59  0.52  42 -0.21   9   7.2 -3.0  552.0   \n",
      "108  2020.12.29 18:00:00  0.6  0.98  0.71  16 -0.19   9   6.8 -3.4  509.0   \n",
      "\n",
      "       10  \n",
      "0   -16.0  \n",
      "1   -12.0  \n",
      "2   -23.0  \n",
      "3    -2.0  \n",
      "4   -20.0  \n",
      "..    ...  \n",
      "104 -12.0  \n",
      "105 -13.0  \n",
      "106 -22.0  \n",
      "107 -14.0  \n",
      "108 -15.0  \n",
      "\n",
      "[1850 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Crear una lista para almacenar los DataFrames de cada año\n",
    "dataframes = []\n",
    "\n",
    "# Rango de años desde 2006 hasta 2021\n",
    "for year in range(2006, 2022):\n",
    "    # Construir la URL con el año actual\n",
    "    url = f\"http://spaceweather.izmiran.ru/dbs/fds/years/{year}.txt\"\n",
    "    \n",
    "    # Realizar una solicitud GET a la URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Leer el contenido de la página como una cadena y separar las líneas\n",
    "        content = response.text\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        # Crear un DataFrame a partir de las líneas con tabulaciones como separadores\n",
    "        df = pd.read_csv(StringIO(content), sep='\\t', header=None)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_combined = pd.concat(dataframes)\n",
    "\n",
    "# Restablecer el índice del DataFrame resultante\n",
    "#df_combined.reset_index(drop=True, inplace=True)\n",
    "#df_combined.to_csv(\"forbush_decrease_database.csv\")\n",
    "# Ahora df_combined contiene los datos de todos los años en un solo DataFrame de pandas\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98deb73-bfca-40f1-a562-8e8757d336f0",
   "metadata": {},
   "source": [
    "### Leyendo el archivo que ya está guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a56bbf-1365-439d-b016-275b7070fe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>MagnM</th>\n",
       "      <th>Axym</th>\n",
       "      <th>Azrange</th>\n",
       "      <th>TminM</th>\n",
       "      <th>DminM</th>\n",
       "      <th>OType</th>\n",
       "      <th>Bmax</th>\n",
       "      <th>Bzmin</th>\n",
       "      <th>Vmax</th>\n",
       "      <th>Dstmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006.01.01 14:05:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>16.8</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>503.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006.01.04 20:00:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.84</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>359.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006.01.06 07:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "      <td>69</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>9</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>401.0</td>\n",
       "      <td>-23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006.01.10 17:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>336.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006.01.15 14:00:00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.30</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>459.0</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2020.12.19 09:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.87</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>388.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2020.12.20 23:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>407.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2020.12.22 01:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>9</td>\n",
       "      <td>12.9</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>637.0</td>\n",
       "      <td>-22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2020.12.27 04:00:00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.52</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2020.12.29 18:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.71</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>509.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  MagnM  Axym  Azrange  TminM  DminM  OType  Bmax  \\\n",
       "0    2006.01.01 14:05:00    0.7  0.90     1.67     -1  -0.29      3  16.8   \n",
       "1    2006.01.04 20:00:00    0.7  1.10     0.84      7  -0.16      9   7.0   \n",
       "2    2006.01.06 07:00:00    0.6  0.94     0.86     69  -0.20      9   8.5   \n",
       "3    2006.01.10 17:00:00    0.6  0.81     0.70     -2  -0.16      9   5.8   \n",
       "4    2006.01.15 14:00:00    1.3  1.04     1.30     28  -0.24      9  12.3   \n",
       "..                   ...    ...   ...      ...    ...    ...    ...   ...   \n",
       "104  2020.12.19 09:00:00    0.5  1.05     0.87     11  -0.21      9   9.0   \n",
       "105  2020.12.20 23:00:00    0.6  0.86     0.92     19  -0.18      9  11.6   \n",
       "106  2020.12.22 01:00:00    0.6  0.82     0.72      6  -0.20      9  12.9   \n",
       "107  2020.12.27 04:00:00    0.5  0.59     0.52     42  -0.21      9   7.2   \n",
       "108  2020.12.29 18:00:00    0.6  0.98     0.71     16  -0.19      9   6.8   \n",
       "\n",
       "     Bzmin   Vmax  Dstmin  \n",
       "0     -2.1  503.0   -16.0  \n",
       "1     -4.5  359.0   -12.0  \n",
       "2     -5.4  401.0   -23.0  \n",
       "3     -4.8  336.0    -2.0  \n",
       "4     -6.1  459.0   -20.0  \n",
       "..     ...    ...     ...  \n",
       "104   -3.3  388.0   -12.0  \n",
       "105   -6.5  407.0   -13.0  \n",
       "106   -2.4  637.0   -22.0  \n",
       "107   -3.0  552.0   -14.0  \n",
       "108   -3.4  509.0   -15.0  \n",
       "\n",
       "[1850 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('forbush_decrease_database.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef82d37-fe3b-47a8-8253-6e60c1152fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>MagnM</th>\n",
       "      <th>Axym</th>\n",
       "      <th>Azrange</th>\n",
       "      <th>TminM</th>\n",
       "      <th>DminM</th>\n",
       "      <th>OType</th>\n",
       "      <th>Bmax</th>\n",
       "      <th>Bzmin</th>\n",
       "      <th>Vmax</th>\n",
       "      <th>Dstmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2006.07.09 21:36:00</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.88</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>438.0</td>\n",
       "      <td>-23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2006.07.27 13:53:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.53</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-8.5</td>\n",
       "      <td>663.0</td>\n",
       "      <td>-47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2006.08.07 00:35:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.44</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1</td>\n",
       "      <td>18.1</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>630.0</td>\n",
       "      <td>-44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2006.08.17 07:20:00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>355.0</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2006.09.04 00:20:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.52</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>621.0</td>\n",
       "      <td>-41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019.05.10 17:54:00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.75</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>382.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019.05.26 22:14:00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.08</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>372.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020.04.20 02:33:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.74</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>-14.3</td>\n",
       "      <td>371.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2020.10.19 14:41:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.93</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>9.1</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2020.10.23 13:20:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>613.0</td>\n",
       "      <td>-38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  MagnM  Axym  Azrange  TminM  DminM  OType  Bmax  \\\n",
       "54  2006.07.09 21:36:00    3.8  1.95     2.88     34  -0.38      1   9.7   \n",
       "60  2006.07.27 13:53:00    1.0  0.82     1.53     14  -0.25      1  14.0   \n",
       "63  2006.08.07 00:35:00    0.9  0.78     1.44     28  -0.38      1  18.1   \n",
       "66  2006.08.17 07:20:00    1.4  1.51     1.29      8  -0.31      1   8.7   \n",
       "74  2006.09.04 00:20:00    0.9  1.25     1.52      4  -0.28      1  14.2   \n",
       "..                  ...    ...   ...      ...    ...    ...    ...   ...   \n",
       "38  2019.05.10 17:54:00    1.6  0.99     0.75     26  -0.28      1  11.9   \n",
       "44  2019.05.26 22:14:00    1.1  0.94     1.08     14  -0.29      1  11.8   \n",
       "38  2020.04.20 02:33:00    0.9  0.96     0.74     10  -0.34      1  15.9   \n",
       "84  2020.10.19 14:41:00    0.6  0.62     0.93     12  -0.19      1   9.1   \n",
       "86  2020.10.23 13:20:00    1.2  0.82     0.63     52  -0.34      1  13.3   \n",
       "\n",
       "    Bzmin   Vmax  Dstmin  \n",
       "54   -6.4  438.0   -23.0  \n",
       "60   -8.5  663.0   -47.0  \n",
       "63   -7.4  630.0   -44.0  \n",
       "66   -6.4  355.0   -17.0  \n",
       "74   -5.1  621.0   -41.0  \n",
       "..    ...    ...     ...  \n",
       "38  -10.5  382.0     NaN  \n",
       "44   -6.3  372.0   -16.0  \n",
       "38  -14.3  371.0   -59.0  \n",
       "84   -3.0  419.0   -13.0  \n",
       "86   -3.5  613.0   -38.0  \n",
       "\n",
       "[185 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = data[data['OType'] == 1]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6371daf3-ed90-4773-aca4-9fd88df86963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2006.07.09 21:36:00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.loc[54]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46c5169-0dc9-4864-b1e6-857a052d61c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   date  MagnM  Axym  Azrange  TminM  DminM  OType  Bmax  \\\n",
      "54  2006.07.09 21:36:00    3.8  1.95     2.88     34  -0.38      1   9.7   \n",
      "60  2006.07.27 13:53:00    1.0  0.82     1.53     14  -0.25      1  14.0   \n",
      "63  2006.08.07 00:35:00    0.9  0.78     1.44     28  -0.38      1  18.1   \n",
      "66  2006.08.17 07:20:00    1.4  1.51     1.29      8  -0.31      1   8.7   \n",
      "74  2006.09.04 00:20:00    0.9  1.25     1.52      4  -0.28      1  14.2   \n",
      "..                  ...    ...   ...      ...    ...    ...    ...   ...   \n",
      "38  2019.05.10 17:54:00    1.6  0.99     0.75     26  -0.28      1  11.9   \n",
      "44  2019.05.26 22:14:00    1.1  0.94     1.08     14  -0.29      1  11.8   \n",
      "38  2020.04.20 02:33:00    0.9  0.96     0.74     10  -0.34      1  15.9   \n",
      "84  2020.10.19 14:41:00    0.6  0.62     0.93     12  -0.19      1   9.1   \n",
      "86  2020.10.23 13:20:00    1.2  0.82     0.63     52  -0.34      1  13.3   \n",
      "\n",
      "    Bzmin   Vmax  Dstmin  continuous_date  \n",
      "54   -6.4  438.0   -23.0     200607092136  \n",
      "60   -8.5  663.0   -47.0     200607271353  \n",
      "63   -7.4  630.0   -44.0     200608070035  \n",
      "66   -6.4  355.0   -17.0     200608170720  \n",
      "74   -5.1  621.0   -41.0     200609040020  \n",
      "..    ...    ...     ...              ...  \n",
      "38  -10.5  382.0     NaN     201905101754  \n",
      "44   -6.3  372.0   -16.0     201905262214  \n",
      "38  -14.3  371.0   -59.0     202004200233  \n",
      "84   -3.0  419.0   -13.0     202010191441  \n",
      "86   -3.5  613.0   -38.0     202010231320  \n",
      "\n",
      "[185 rows x 12 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9641/409621421.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['continuous_date'] = filtered_data['date'].apply(convert_date_to_continuous_number)\n"
     ]
    }
   ],
   "source": [
    "# Importar la biblioteca datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Función para convertir la fecha\n",
    "def convert_date_to_continuous_number(date_str):\n",
    "    # Convertir la fecha en un objeto datetime\n",
    "    date_obj = datetime.strptime(date_str, '%Y.%m.%d %H:%M:%S')\n",
    "    \n",
    "    # Construir un número continuo a partir de la fecha\n",
    "    continuous_number = int(date_obj.strftime('%Y%m%d%H%M'))\n",
    "    \n",
    "    return continuous_number\n",
    "\n",
    "# Aplicar la función a la columna 'date' en filtered_data\n",
    "filtered_data['continuous_date'] = filtered_data['date'].apply(convert_date_to_continuous_number)\n",
    "\n",
    "# Verificar los resultados\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cadd61f-d7c9-4cc3-a329-432424cefadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200607092136"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['continuous_date'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3d8b0-0c67-4d48-81a9-564cbe7e6f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1800b392-a99f-4449-acef-6c0526bf52c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n",
      "/tmp/ipykernel_9641/432665941.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_data[date] = data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Tomar el primer enlace\n",
    "first_url = f\"http://spaceweather.izmiran.ru/dbs/fds/events/{filtered_data['continuous_date'].iloc[0]}.txt\"\n",
    "\n",
    "# Realizar una solicitud GET al primer enlace\n",
    "response = requests.get(first_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Leer el contenido del primer enlace como una lista de líneas\n",
    "    lines = response.text.splitlines()\n",
    "    \n",
    "    # Tomar la primera columna del primer dataset y convertirla en el encabezado\n",
    "    header = lines[0].split('\\t')[0]\n",
    "    \n",
    "    # Crear un DataFrame vacío con el encabezado\n",
    "    new_data = pd.DataFrame(columns=[header])\n",
    "    \n",
    "    # Iterar a través de las fechas en la columna 'date' de filtered_data\n",
    "    for date in filtered_data['continuous_date']:\n",
    "        # Construir la URL correspondiente\n",
    "        url = f\"http://spaceweather.izmiran.ru/dbs/fds/events/{date}.txt\"\n",
    "        \n",
    "        # Realizar una solicitud GET a la URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Leer el contenido de la página como una lista de líneas\n",
    "            lines = response.text.splitlines()\n",
    "            \n",
    "            # Tomar la segunda columna de cada dataset y añadirla como una nueva columna al DataFrame\n",
    "            data = [line.split('\\t')[1] for line in lines]\n",
    "            \n",
    "            # Agregar una nueva columna al DataFrame\n",
    "            new_data[date] = data\n",
    "\n",
    "# Restablecer el índice del nuevo DataFrame\n",
    "new_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "257f26d5-18b3-40f7-8acb-b94253409847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>200607092136</th>\n",
       "      <th>200607271353</th>\n",
       "      <th>200608070035</th>\n",
       "      <th>200608170720</th>\n",
       "      <th>200609040020</th>\n",
       "      <th>200612080435</th>\n",
       "      <th>200612141414</th>\n",
       "      <th>200612161755</th>\n",
       "      <th>200612181014</th>\n",
       "      <th>...</th>\n",
       "      <th>201804200021</th>\n",
       "      <th>201805051025</th>\n",
       "      <th>201805221218</th>\n",
       "      <th>201805311446</th>\n",
       "      <th>201809290053</th>\n",
       "      <th>201905101754</th>\n",
       "      <th>201905262214</th>\n",
       "      <th>202004200233</th>\n",
       "      <th>202010191441</th>\n",
       "      <th>202010231320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-09</td>\n",
       "      <td>2006-07-27</td>\n",
       "      <td>2006-08-07</td>\n",
       "      <td>2006-08-17</td>\n",
       "      <td>2006-09-04</td>\n",
       "      <td>2006-12-08</td>\n",
       "      <td>2006-12-14</td>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>2018-05-05</td>\n",
       "      <td>2018-05-22</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>2018-09-29</td>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>2020-10-19</td>\n",
       "      <td>2020-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21:36:00</td>\n",
       "      <td>13:53:00</td>\n",
       "      <td>0:35:00</td>\n",
       "      <td>7:20:00</td>\n",
       "      <td>0:20:00</td>\n",
       "      <td>4:35:00</td>\n",
       "      <td>14:14:00</td>\n",
       "      <td>17:55:00</td>\n",
       "      <td>10:14:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0:21:00</td>\n",
       "      <td>10:25:00</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>14:46:00</td>\n",
       "      <td>0:53:00</td>\n",
       "      <td>17:54:00</td>\n",
       "      <td>22:14:00</td>\n",
       "      <td>2:33:00</td>\n",
       "      <td>14:41:00</td>\n",
       "      <td>13:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-07-06</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-12-06</td>\n",
       "      <td>2006-12-13</td>\n",
       "      <td>2006-12-14</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8:13:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18:29:00</td>\n",
       "      <td>2:14:00</td>\n",
       "      <td>21:07:00</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>90.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>222.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>93</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Date       200607092136 200607271353 200608070035 200608170720  \\\n",
       "0          NaN   2006-07-09   2006-07-27   2006-08-07   2006-08-17   \n",
       "1          NaN     21:36:00     13:53:00      0:35:00      7:20:00   \n",
       "2          NaN            1            1            1            1   \n",
       "3          NaN   2006-07-06         None         None         None   \n",
       "4          NaN      8:13:00         None         None         None   \n",
       "..         ...          ...          ...          ...          ...   \n",
       "108        NaN         90.5         None         None         None   \n",
       "109        NaN           93            0            0           95   \n",
       "110        NaN         None            1            1         None   \n",
       "111        NaN            0            0            0            0   \n",
       "112        NaN           31           18            0           27   \n",
       "\n",
       "    200609040020 200612080435 200612141414 200612161755 200612181014  ...  \\\n",
       "0     2006-09-04   2006-12-08   2006-12-14   2006-12-16   2006-12-18  ...   \n",
       "1        0:20:00      4:35:00     14:14:00     17:55:00     10:14:00  ...   \n",
       "2              1            1            1            1            1  ...   \n",
       "3           None   2006-12-06   2006-12-13   2006-12-14         None  ...   \n",
       "4           None     18:29:00      2:14:00     21:07:00         None  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "108         None         None        222.3         None         None  ...   \n",
       "109            0           94           77           77            0  ...   \n",
       "110            1         None         None         None            1  ...   \n",
       "111            0            0           92            0            0  ...   \n",
       "112            0           28           23           20            0  ...   \n",
       "\n",
       "    201804200021 201805051025 201805221218 201805311446 201809290053  \\\n",
       "0     2018-04-20   2018-05-05   2018-05-22   2018-05-31   2018-09-29   \n",
       "1        0:21:00     10:25:00     12:18:00     14:46:00      0:53:00   \n",
       "2              1            1            1            1            1   \n",
       "3           None         None         None         None         None   \n",
       "4           None         None         None         None         None   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "108         None         None         None         None         None   \n",
       "109         None         None         None         None         None   \n",
       "110         None         None         None         None         None   \n",
       "111            0            0            0            0            0   \n",
       "112         None         None         None         None         None   \n",
       "\n",
       "    201905101754 201905262214 202004200233 202010191441 202010231320  \n",
       "0     2019-05-10   2019-05-26   2020-04-20   2020-10-19   2020-10-23  \n",
       "1       17:54:00     22:14:00      2:33:00     14:41:00     13:20:00  \n",
       "2              1            1            1            1            1  \n",
       "3           None         None         None         None         None  \n",
       "4           None         None         None         None         None  \n",
       "..           ...          ...          ...          ...          ...  \n",
       "108         None         None         None         None         None  \n",
       "109         None           93         None         None         None  \n",
       "110         None         None         None         None         None  \n",
       "111            0            0            0            0            0  \n",
       "112         None         None         None         None         None  \n",
       "\n",
       "[113 rows x 186 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de5295ff-9be1-4289-b1d4-9e684e5a9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Tomar el primer enlace\n",
    "first_url = f\"http://spaceweather.izmiran.ru/dbs/fds/events/{filtered_data['continuous_date'].iloc[0]}.txt\"\n",
    "\n",
    "# Realizar una solicitud GET al primer enlace\n",
    "response = requests.get(first_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d91f3c-f384-43ae-aa6d-76fd0be04a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date      \t2006-07-09\n",
      "Time      \t21:36:00\n",
      "Otype      \t1\n",
      "Sdate      \t2006-07-06\n",
      "Stime      \t8:13:00\n",
      "Stype      \t1.0\n",
      "Source      \tSF+CH\n",
      "Qs      \t4\n",
      "Vsp      \t1043.0\n",
      "Vmean      \t487.0\n",
      "VmeanC      \t486\n",
      "Vmax      \t438.0\n",
      "tVmax      \t2\n",
      "Bmax      \t9.7\n",
      "tBmax      \t3\n",
      "VmBm      \t2.12\n",
      "Bzmin      \t-6.4\n",
      "ABzmax      \t7.2\n",
      "BzmtoBm      \t0.742\n",
      "KTmax      \t1.919\n",
      "tKTmax      \t3.0\n",
      "KTmin      \t0.262\n",
      "tKTmin      \t28.0\n",
      "Bemax      \t25.82\n",
      "tBemax      \t8.0\n",
      "Bemin      \t0.38\n",
      "tBemin      \t26.0\n",
      "Rbulk      \t15.23\n",
      "B2Sum      \t1.31\n",
      "Magn      \t4.5\n",
      "MagnM      \t3.8\n",
      "MagnL      \t11.82\n",
      "MagnC      \t4.5\n",
      "Tmin      \t34\n",
      "TminM      \t34\n",
      "Dmin      \t-0.51\n",
      "tDmin      \t22\n",
      "DminM      \t-0.38\n",
      "tDminM      \t4\n",
      "GrDmin      \t55.69\n",
      "Dmax      \t0.27\n",
      "tDmax      \t25\n",
      "AftoB      \t0.464\n",
      "GammaM      \t0.84\n",
      "GammaD      \t0.75\n",
      "tBef      \t142.0\n",
      "tAft      \t37.0\n",
      "tFrom      \t37.0\n",
      "Kpmax      \t3.33\n",
      "Apmax      \t18.0\n",
      "Dstmin      \t-23.0\n",
      "tDstmin      \t18.0\n",
      "Fdata      \tM 2.5/SX\n",
      "Xmagn      \t2.5e-05\n",
      "Hlat      \t-9.0\n",
      "Alat      \t9.0\n",
      "Hlon      \t34.0\n",
      "Sdur      \t38\n",
      "Axym      \t1.95\n",
      "Txymax      \t26.0\n",
      "Axm      \t0.03\n",
      "Aym      \t1.95\n",
      "Azrange      \t2.88\n",
      "Pxym      \t89.1\n",
      "Axdm      \t-0.45\n",
      "Aydm      \t-0.27\n",
      "Azdm      \t-0.33\n",
      "Axydm      \t0.52\n",
      "Pxydm      \t211.0\n",
      "D01      \t-0.33\n",
      "D02      \t-0.36\n",
      "AD02      \t0.36\n",
      "AxytoAf      \t0.31\n",
      "Axb      \t0.61\n",
      "Ayb      \t1.39\n",
      "Azb      \t0.46\n",
      "Axyb      \t1.52\n",
      "Pxyb      \t60.8\n",
      "Ax0      \t0.66\n",
      "Ay0      \t1.0\n",
      "Az0      \t0.04\n",
      "Axy0      \t1.2\n",
      "Pxy0      \t56.6\n",
      "Ax1      \t0.61\n",
      "Ay1      \t1.39\n",
      "Az1      \t0.43\n",
      "Axy1      \t1.52\n",
      "Pxy1      \t66.3\n",
      "dx1b      \t-0.15\n",
      "dy1b      \t0.03\n",
      "dz1b      \t-0.03\n",
      "dxy1b      \t0.15\n",
      "dA11b      \t0.16\n",
      "dz1b      \t-0.03\n",
      "Adz1b      \t0.03\n",
      "dPxy1b      \t-102.4\n",
      "Pdxy1b      \t168.7\n",
      "DP1b      \t5.3\n",
      "CMEdate      \t2006-07-06\n",
      "CMEtime      \t8:54:04\n",
      "CMEwidth      \t360\n",
      "CMEangle      \t-999\n",
      "MCstartdate      \tNone\n",
      "MCstarttime      \tNone\n",
      "MCdur      \t-1\n",
      "RMC      \tNone\n",
      "EruptA      \t33.9\n",
      "EruptD      \t56.6\n",
      "EruptAD      \t90.5\n",
      "Ind      \t93\n",
      "Spol      \tNone\n",
      "GLE      \t0\n",
      "SSN      \t31\n",
      "\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 2 fields in line 53, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_content)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Para convertir el contenido en un DataFrame\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Eliminar espacios en los valores de la primera columna\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:286\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    283\u001b[0m     indexnamerow \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m     content \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 286\u001b[0m alldata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rows_to_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[1;32m    289\u001b[0m conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:1058\u001b[0m, in \u001b[0;36mPythonParser._rows_to_cols\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m   1052\u001b[0m             reason \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1053\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError could possibly be due to quotes being \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1054\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored when a multi-char delimiter is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m             )\n\u001b[1;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reason\n\u001b[0;32m-> 1058\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_alert_malformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# see gh-13320\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m zipped_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lib\u001b[38;5;241m.\u001b[39mto_object_array(content, min_width\u001b[38;5;241m=\u001b[39mcol_len)\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:779\u001b[0m, in \u001b[0;36mPythonParser._alert_malformed\u001b[0;34m(self, msg, row_num)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mERROR:\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mWARN:\n\u001b[1;32m    781\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mParserError\u001b[0m: Expected 2 fields in line 53, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    # Leer el contenido de la respuesta\n",
    "    file_content = response.text\n",
    "\n",
    "    # Puedes imprimir el contenido para verificar\n",
    "    print(file_content)\n",
    "\n",
    "    # Para convertir el contenido en un DataFrame\n",
    "    data = pd.read_csv(StringIO(file_content), sep='\\s+', engine='python')\n",
    "    # Eliminar espacios en los valores de la primera columna\n",
    "    data.iloc[:, 0] = data.iloc[:, 0].str.strip()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c5a86d6-aaac-4ef3-abd2-941d293609b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time      ',\n",
       " 'Otype      ',\n",
       " 'Sdate      ',\n",
       " 'Stime      ',\n",
       " 'Stype      ',\n",
       " 'Source      ',\n",
       " 'Qs      ',\n",
       " 'Vsp      ',\n",
       " 'Vmean      ',\n",
       " 'VmeanC      ',\n",
       " 'Vmax      ',\n",
       " 'tVmax      ',\n",
       " 'Bmax      ',\n",
       " 'tBmax      ',\n",
       " 'VmBm      ',\n",
       " 'Bzmin      ',\n",
       " 'ABzmax      ',\n",
       " 'BzmtoBm      ',\n",
       " 'KTmax      ',\n",
       " 'tKTmax      ',\n",
       " 'KTmin      ',\n",
       " 'tKTmin      ',\n",
       " 'Bemax      ',\n",
       " 'tBemax      ',\n",
       " 'Bemin      ',\n",
       " 'tBemin      ',\n",
       " 'Rbulk      ',\n",
       " 'B2Sum      ',\n",
       " 'Magn      ',\n",
       " 'MagnM      ',\n",
       " 'MagnL      ',\n",
       " 'MagnC      ',\n",
       " 'Tmin      ',\n",
       " 'TminM      ',\n",
       " 'Dmin      ',\n",
       " 'tDmin      ',\n",
       " 'DminM      ',\n",
       " 'tDminM      ',\n",
       " 'GrDmin      ',\n",
       " 'Dmax      ',\n",
       " 'tDmax      ',\n",
       " 'AftoB      ',\n",
       " 'GammaM      ',\n",
       " 'GammaD      ',\n",
       " 'tBef      ',\n",
       " 'tAft      ',\n",
       " 'tFrom      ',\n",
       " 'Kpmax      ',\n",
       " 'Apmax      ',\n",
       " 'Dstmin      ',\n",
       " 'tDstmin      ',\n",
       " 'Fdata      ',\n",
       " 'Xmagn      ',\n",
       " 'Hlat      ',\n",
       " 'Alat      ',\n",
       " 'Hlon      ',\n",
       " 'Sdur      ',\n",
       " 'Axym      ',\n",
       " 'Txymax      ',\n",
       " 'Axm      ',\n",
       " 'Aym      ',\n",
       " 'Azrange      ',\n",
       " 'Pxym      ',\n",
       " 'Axdm      ',\n",
       " 'Aydm      ',\n",
       " 'Azdm      ',\n",
       " 'Axydm      ',\n",
       " 'Pxydm      ',\n",
       " 'D01      ',\n",
       " 'D02      ',\n",
       " 'AD02      ',\n",
       " 'AxytoAf      ',\n",
       " 'Axb      ',\n",
       " 'Ayb      ',\n",
       " 'Azb      ',\n",
       " 'Axyb      ',\n",
       " 'Pxyb      ',\n",
       " 'Ax0      ',\n",
       " 'Ay0      ',\n",
       " 'Az0      ',\n",
       " 'Axy0      ',\n",
       " 'Pxy0      ',\n",
       " 'Ax1      ',\n",
       " 'Ay1      ',\n",
       " 'Az1      ',\n",
       " 'Axy1      ',\n",
       " 'Pxy1      ',\n",
       " 'dx1b      ',\n",
       " 'dy1b      ',\n",
       " 'dz1b      ',\n",
       " 'dxy1b      ',\n",
       " 'dA11b      ',\n",
       " 'dz1b      ',\n",
       " 'Adz1b      ',\n",
       " 'dPxy1b      ',\n",
       " 'Pdxy1b      ',\n",
       " 'DP1b      ',\n",
       " 'CMEdate      ',\n",
       " 'CMEtime      ',\n",
       " 'CMEwidth      ',\n",
       " 'CMEangle      ',\n",
       " 'MCstartdate      ',\n",
       " 'MCstarttime      ',\n",
       " 'MCdur      ',\n",
       " 'RMC      ',\n",
       " 'EruptA      ',\n",
       " 'EruptD      ',\n",
       " 'EruptAD      ',\n",
       " 'Ind      ',\n",
       " 'Spol      ',\n",
       " 'GLE      ',\n",
       " 'SSN      ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Seleccionar la primera columna y convertirla en una lista de strings\n",
    "columna1_lista = data.iloc[:, 0].astype(str).tolist()\n",
    "columna1_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59a3f664-c0ea-4653-8cf3-9355f86c45b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date      ]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    # Leer el contenido del primer enlace como una lista de líneas\n",
    "    lines = response.text.splitlines()\n",
    "    \n",
    "    # Tomar la primera columna del primer dataset y convertirla en el encabezado\n",
    "    header = lines[0].split('\\t')[0]\n",
    "    \n",
    "    # Crear un DataFrame vacío con el encabezado\n",
    "    new_data = pd.DataFrame(columns=[header])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90b45c81-07b2-43fc-803f-69b03a197773",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Expected 2 fields in line 53, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m file_content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Para convertir el contenido en un DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Utilizar una expresión regular para eliminar cualquier caracter no deseado en la primera columna\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, x))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:286\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    283\u001b[0m     indexnamerow \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m     content \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 286\u001b[0m alldata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rows_to_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[1;32m    289\u001b[0m conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:1058\u001b[0m, in \u001b[0;36mPythonParser._rows_to_cols\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m   1052\u001b[0m             reason \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1053\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError could possibly be due to quotes being \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1054\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored when a multi-char delimiter is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m             )\n\u001b[1;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reason\n\u001b[0;32m-> 1058\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_alert_malformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# see gh-13320\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m zipped_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lib\u001b[38;5;241m.\u001b[39mto_object_array(content, min_width\u001b[38;5;241m=\u001b[39mcol_len)\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:779\u001b[0m, in \u001b[0;36mPythonParser._alert_malformed\u001b[0;34m(self, msg, row_num)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mERROR:\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mWARN:\n\u001b[1;32m    781\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mParserError\u001b[0m: Expected 2 fields in line 53, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Leer el contenido de la respuesta\n",
    "    file_content = response.text\n",
    "\n",
    "    # Para convertir el contenido en un DataFrame\n",
    "    data = pd.read_csv(StringIO(file_content), sep='\\s+', engine='python')\n",
    "\n",
    "    # Utilizar una expresión regular para eliminar cualquier caracter no deseado en la primera columna\n",
    "    data.iloc[:, 0] = data.iloc[:, 0].apply(lambda x: re.sub(r'\\W', '', x))\n",
    "\n",
    "# Seleccionar la primera columna y convertirla en una lista de strings\n",
    "columna1_lista = data.iloc[:, 0].astype(str).tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
